```{r setup, echo=FALSE}
opts_chunk$set(cache=FALSE, tidy=TRUE, echo=FALSE, highlight=TRUE, figalign="center", fig.height=8.5, fig.width=8.5, message=FALSE, error=FALSE, warning=FALSE)
```


```{r libraries}
library(DESeq)
library(plyr)
library(reshape)
library(ggplot2)
library(xtable)
library(biomaRt)
library(scales)
ensembl = useMart("ensembl",dataset="mmusculus_gene_ensembl")
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")
```

## Methods summary
All reads were trimmed to remove adapter sequences and low quality calls.  Trimmed reads were aligned with Tophat2[^1] and gene specific read counts for iGenomes Ensembl gene annotations[^2] determined with htseq-count[^3]. Read counts were then normalized and differential gene expression calculated with the DESeq Bioconductor package[^4]. The cutoff for significant differential expression was an FDR (False discovery rate) of 0.2 . 

```{r variables }
if (file.exists("/n/hsphS10/hsphfs1/hsci/Vaidya/vv_miRNA-Seq")) {
  baseDir <- "/n/hsphS10/hsphfs1/hsci/Vaidya/vv_miRNA-Seq"
} else {
  baseDir <- "/Volumes/ody/consults/vv_miRNA-Seq"
}
dataDir <- file.path(baseDir, "results/htseq-count/")
resultsDir <- file.path(baseDir, "results/deseq")
metaDir <- file.path(baseDir, "meta")

count.file <- file.path(dataDir, "combined.counts")
pvalcutoff=0.2
numsig=10

gene_symbol = 'mgi_symbol'
ensembl_gene = 'mmusculus_gene_ensembl'
filter_type = 'ensembl_gene_id'
```

```{r functions }
annotate_df = function(d) {
  require(biomaRt)
	ensembl = useMart('ensembl', dataset = ensembl_gene)
	a = getBM(attributes=c(filter_type, gene_symbol, "description"),
		filters=c(filter_type), values=d[, 'id'],
		mart=ensembl)
	m = merge(d, a, by.x='id', by.y=filter_type)
	return(m)
}

plotDispEsts = function(cds) {
  estimates = data.frame(means = rowMeans(counts(cds, normalized=TRUE)),
		variance = fitInfo(cds)$perGeneDispEsts)
	xg = 10^seq(-0.5, 5, length.out=300)
	yg = fitInfo(cds)$dispFun(xg)
	fitline = data.frame(xg=xg, yg=yg)
	p = ggplot(estimates, aes(means, variance)) + geom_point(size=1, alpha=0.4) +
		scale_x_log10() + scale_y_log10() +
		geom_line(data=fitline, aes(xg, yg), color="red") +
		labs(title="dispersion estimation while pooling all samples") +
		xlab("mean number of mapped reads per gene") +
		ylab("estimated dispersion")
	p
}

lm_eqn = function(df){
    m = lm(rep.2 ~ rep.1, df);
    eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2, 
         list(a = format(coef(m)[1], digits = 2), 
              b = format(coef(m)[2], digits = 2), 
             r2 = format(summary(m)$r.squared, digits = 3)))
    as.character(as.expression(eq));                 
}

qq = function(pvaldf,  cutoffpval, samples) {
  title=paste("Quantile-quantile plot of p-values", samples, sep=" - ")
  pvaldf <- pvaldf[order(pvaldf$pval, decreasing=F),]
  pvals <- as.vector(unlist(pvaldf$pval))
  padjs <- as.numeric(as.vector(unlist(pvaldf$padj)))
  colors <- as.vector(ifelse(padjs<cutoffpval, "sig", "nonsig"))
  o = -log10(pvals)
  e = -log10( 1:length(o)/length(o) )
  plot=qplot(e,o, color=colors, xlim=c(0,max(e[!is.na(e)])), ylim=c(0,max(o[!is.na(o)]))) + stat_abline(intercept=0,slope=1, col="darkgrey")
  plot=plot+labs(title=title)
  plot=plot+scale_x_continuous(name=expression(Expected~~-log[10](italic(p))))
  plot=plot+scale_y_continuous(name=expression(Observed~~-log[10](italic(p))))
  plot=plot + scale_colour_manual(name="BFH adjusted pvalue", values=c("black", "red"), labels=c(paste("q>", cutoffpval, sep=""),paste("q<", cutoffpval,sep=""))) 
  plot
}

plotDE <- function(res, cutoffpval, samples ) {
  title=paste("M-A plot of", samples, sep=" - ")
  res$colors <- ifelse(res$padj<cutoffpval, "sig", "nonsig" )
  plot <- ggplot(data=res, aes(x=log(baseMean), y=log2(foldChange), colour=colors)) + 
    geom_point( size=3)  +  
    scale_colour_manual(name="BFH adjusted pvalue", values=c("#00000033","#FF0000FF"),labels=c(paste("q>", cutoffpval, sep=""),paste("q<", cutoffpval,sep=""))) +
    labs(title=title)
  plot
}
```

## DATALOAD and PREP
- HTseq counts were generated for individual genes (using the Ensembl GTF annotation) from the Tophat aligments
- covariates are "Normal" (no treatment) and then days past treatment ("FA#day"), 3 replicates per time point
- processed and loaded all data into a DESeq CountDataSet

```{r dataload_and_reshape}
counts <- read.table(file.path(dataDir, "combined.counts"), header=T, row.names=1)
counts <- counts[, order(names(counts), decreasing=T)]
# drop directories and additional trailing info from sample names, leave covariate and replicate info only 
names(counts) <- sub("results.htseq.count.", "", names(counts))
names(counts) <- sub("_.+$","", names(counts) )

# use samplenames get covars from the filenames of the counted samples
# identical values in this vector will be marked as replicates
covars <- factor(sub("rep.+$", "", names(counts)))

## load up new count dataset
cds <- newCountDataSet(counts, covars)
```

## NORMALIZATION

Here is a sample of `r numsig` of the non-normalized counts of reads mapping to
genes in the annotation. These are just chosen at random for display purposes:

```{r rawcounts, results='asis'}
annotated_counts = head(as.data.frame(counts(cds)), numsig)
annotated_counts$id = rownames(annotated_counts)
annotated_counts = annotate_df(annotated_counts)
print(xtable(annotated_counts), type='html', include.rownames=F)
```

To normalize for different depth of sequencing, size factors are calculated
for each sample.
Call $k_{ij}$ the count for gene $i$ in sample $j$.
For all samples $j$, assume most of the counts for genes will have
a constant ratio comparing any two samples in $j$, reflecting the assumption
that most genes are not differentially expressed between samples.
A reference set of counts is created by taking $f_i$, geometric mean of the counts for each
gene $i$, and the median of of $\frac{k_i}{f_i}$ for sample $j$ is called the size
factor for sample $j$. The size factors can be thought of as an attempt to
normalize for the total number of reads mapping to transcripts, depending on the assumption that most
of the transcripts are not vastly different between samples.

Here are the size factors for each sample in this experiment:

```{r sizefactors, tidy=TRUE}
cds = estimateSizeFactors(cds)
sizeFactors(cds)
```

These are the normalized counts, which are the raw counts divided by
the size factor, for the same `r numsig` genes:

```{r normalized_counts, results='asis'}
annotated_normalized = head(as.data.frame(counts(cds, normalized=TRUE)), numsig)
annotated_normalized$id = rownames(annotated_normalized)
annotated_normalized = annotate_df(annotated_normalized)
print(xtable(annotated_normalized), type='html', include.rownames=F)
```

We can see the assumption holds true by plotting the ratio of counts for 
any two randomly drawn samples and noting the distribution has a large central peak.

In blue are the raw counts and in red are the normalized counts.
The effect of normalization is to shift the mean ratio towards 1.

```{r ratio_hist, fig.cap=""}
raw.counts = counts(cds, normalized=FALSE)
cols <- sample(ncol(raw.counts),2, replace=F)
rawdata = data.frame(ratio=raw.counts[,cols[1]] / raw.counts[,cols[2]])
rawdata$set <- "raw"
norm.counts = counts(cds, normalized=TRUE)
normdata = data.frame(ratio=norm.counts[,1] / norm.counts[,2])
normdata$set <- "normalized"
raw.norm.data <- rbind(rawdata, normdata)

n = ggplot(raw.norm.data, aes(x=ratio, fill=set)) + geom_density(alpha=0.25) +
	scale_x_log10(breaks=c(0.01, 0.1, 1, 10, 100), labels=math_format(format=log10)) +
	labs(title="Normalized counts")
print(n)
```

## Variance estimation
With replicates you can get a more accurate repesentation 
of the biological variability or dispersion. Dispersion describes 
the fluctuation of the observed counts for a gene
around the mean counts for that gene and can be the square of the
coefficient of biological variation.
For example, if a gene's expression level fluctuates by 20% the
calculated dispersion is
$0.2^2 = 0.04$.

Below is a plot of the estimated dispersion for genes with given counts;
the dispersion increases as the mean count decreases meaning it is
more difficult to accurately measure the expression level of
low-count genes.

```{r estimate_sizefactors, results='hide', fig.cap="Empirical and fitted dispersion values plotted against mean expression strength"}
# sharingMode = maximum, most conservative approach to sharing information across genes to reduce variability of the dispersion estimates
cds <- estimateDispersions(cds, method="pooled", sharingMode="maximum", fitType="parametric")
plotDispEsts(cds)
```

## QC

### Primary Clustering
- the goal of this analysis is to naiively evaluate the variability within the raw data and determine whether this variability can predict the different sample categories
- Principal Component Analysis  (PCA) is a dimension reduction and visualisation technique that is used to project the multivariate data vector of each sample into a two-dimensional plot, such that the spatial arrangement of the points in the plot reflects the overall data (dis)similarity between the samples.
- in basic terms, we are checking whether the samples cluster by sample type; its a good sign if they do

The plots show a scatterplot of the samples along all dual combinations of the first four principal components. Each sample group is represented by a separate color. You can use these plots to explore if the samples cluster, and whether this is according to an intended experimental factor or according to unintended causes such as batch effects. 

```{r pca}
pcaPlot <- function(readdata, title)  {
  fit <- prcomp(t(readdata))
  colors <- cbPalette[factor(pData(cds)$condition)]
  legend_values=unique(cbind(colors, as.character(pData(cds)$condition)))
  ##all samples
  plot(fit$x, bg=colors, col="black", cex=2,pch=21, main=title, oma=c(8,5,5,14))
  legend("topright", cex=0.7, col="black", pt.bg=legend_values[,1], pt.cex=1.25, legend=legend_values[,2],  pch=21, bty="n", x.intersp=1)
  }
pcaPlot(raw.counts, "Raw counts")
pcaPlot(norm.counts, "Normalized counts")
```

## ANALYSES

### Filtering genes with no variance in samples
- using this data, I performed independent filtering to eliminate genes that have no, or little chance of showing signicant evidence
  - this should result in increased detection power, in terms of false discovery rate.
  - filtering was based on the sum of counts from all samples
    - below a certain number of counts, it is not possible to get a pvalue below a desired cutoff 
    - here I discarded the genes in the bottom 40% of the distribution

```{r filter}
  ## get sum of counts for all samples for each gene
  rowcounts <- rowSums(norm.counts)
  ## filter the data based on the minimal row sum 
  use <- (rowcounts > quantile(rowcounts, 0.4))
  cds.filt <- cds[use,]
  filt.norm.counts <- norm.counts[use,]
```

Run each pairwise analysis, comparing each day's results to the "Normal" samples

```{r pairwise_comparisons, cache=TRUE}
## first construct the actual combinations
all.pair.combos <- combn(as.vector(unique(pData(cds)$condition)),2)
vs.norm.combos <- all.pair.combos[,which(all.pair.combos[1,]=="Normal")]
setnames <- apply(vs.norm.combos, 2, function(n) paste(n[1], n[2], sep="-vs-"))
sig.results <- alply(vs.norm.combos, 2, function(combo) {
  setname <- paste(combo[1], combo[2], sep="-vs-")
  print(setname)
  ## perform significance testing
  res.filt <- nbinomTest(cds.filt, combo[1], combo[2])
  ## get normalized counts for significant hits, relabel samples with condition rather than sampleID
  filt.norm.counts.d <- as.data.frame(filt.norm.counts)
  results.1 <- filt.norm.counts.d[which(res.filt$padj<pvalcutoff),]
  ## get means and pvalues for significant hits and put together with counts
  results.2 <- res.filt[which(res.filt$padj<pvalcutoff),]
  results <- cbind(results.1, results.2)
  results <- annotate_df(results)
  results <- results[order(results$padj),]
  ## output some plots
  qqplots <- qq(res.filt[,c("pval", "padj")], pvalcutoff, setname)
  DEplots <- plotDE(res.filt, pvalcutoff, setname)
  return(list(res.filt=res.filt, results=results,  qqplots=qqplots, DEplots=DEplots))
})  

sig.results.unfilt <- alply(vs.norm.combos, 2, function(combo) {
  setname <- paste(combo[1], combo[2], sep="-vs-")
  print(setname)
  ## perform significance testing
  res.unfilt <- nbinomTest(cds, combo[1], combo[2])
  return(list(res.unfilt=res.unfilt))
}) 

```



**6 pairwise comparisons in total**

- for each comparison, significant genes which passed a `r pvalcutoff*100`% false discovery rate are reported or highlighted

- for each comparison, there are 3 ways to visualize these significant results:
  - QQplot, with significantly varying transcripts shaded in red
  - MA-plot, with significantly varying transcripts shaded in red
  - table of transcripts with significantly different expression levels 

---

### Normal vs. Day1 post Folic Acid Treatment
 
```{r out1, fig.width=11, fig.height=6}
sig.results[[5]]$qqplots
sig.results[[5]]$DEplots
```
**QQ and M-A plots highlighting genes with significant differential expression**  

**Table of top 10 transcripts showing significant (FDR<0.2) differential expression** 
```{r tables1, results='asis'}
if (nrow(sig.results[[5]]$results)>(numsig-1)) {
  out <- xtable(sig.results[[5]]$results[1:numsig,])
  } else {
    out  <- xtable(sig.results[[5]]$results)
    }
print(out, type='html',include.rownames=FALSE)
write.table(sig.results[[5]]$results, file=file.path(resultsDir, paste("DE.genes.q", pvalcutoff, setnames[5], "xls", sep=".")), quote=F, sep="\t", row.names=F, col.names=T)
```

[Significant results](editlink
./results/deseq/`r paste("DE.genes.q", pvalcutoff, setnames[5], "xls", sep=".")`

---

### Normal vs. Day2 post Folic Acid Treatment

```{r out2, fig.width=11, fig.height=6}
sig.results[[3]]$qqplots
sig.results[[3]]$DEplots
```
**QQ and M-A plots highlighting genes with significant differential expression**  

**Table of transcripts showing significant (FDR<0.2) differential expression** 
```{r tables2, results='asis'}
if (nrow(sig.results[[3]]$results)>(numsig-1)) {
  out <- xtable(sig.results[[3]]$results[1:numsig,])
  } else {
    out  <- xtable(sig.results[[3]]$results)
    }
print(out, type='html',include.rownames=FALSE)
write.table(sig.results[[3]]$results, file=file.path(resultsDir, paste("DE.genes.q", pvalcutoff, setnames[3], "xls", sep=".")), quote=F, sep="\t", row.names=F, col.names=T)
```

[Significant results](editlink
./results/deseq/`r paste("DE.genes.q", pvalcutoff, setnames[3], "xls", sep=".")`

---

### Normal vs. Day3 post Folic Acid Treatment

```{r out3, fig.width=11, fig.height=6}
sig.results[[2]]$qqplots
sig.results[[2]]$DEplots
```
**QQ and M-A plots highlighting genes with significant differential expression**  

**Table of transcripts showing significant (FDR<0.2) differential expression** 
```{r tables3, results='asis'}
if (nrow(sig.results[[2]]$results)>(numsig-1)) {
  out <- xtable(sig.results[[2]]$results[1:numsig,])
  } else {
    out  <- xtable(sig.results[[2]]$results)
    }
print(out, type='html',include.rownames=FALSE)
write.table(sig.results[[2]]$results, file=file.path(resultsDir, paste("DE.genes.q", pvalcutoff, setnames[2], "xls", sep=".")), quote=F, sep="\t", row.names=F, col.names=T)
```

[Significant results](editlink
./results/deseq/`r paste("DE.genes.q", pvalcutoff, setnames[2], "xls", sep=".")`

---

### Normal vs. Day7 post Folic Acid Treatment

```{r out4, fig.width=11, fig.height=6}
sig.results[[1]]$qqplots
sig.results[[1]]$DEplots
```
**QQ and M-A plots highlighting genes with significant differential expression**  

**Table of transcripts showing significant (FDR<0.2) differential expression** 
```{r tables4, results='asis' }
if (nrow(sig.results[[1]]$results)>(numsig-1)) {
  out <- xtable(sig.results[[1]]$results[1:numsig,])
  } else {
    out  <- xtable(sig.results[[1]]$results)
    }
print(out, type='html',include.rownames=FALSE)
write.table(sig.results[[1]]$results, file=file.path(resultsDir, paste("DE.genes.q", pvalcutoff, setnames[1], "xls", sep=".")), quote=F, sep="\t", row.names=F, col.names=T)
```

[Significant results](editlink
./results/deseq/`r paste("DE.genes.q", pvalcutoff, setnames[1], "xls", sep=".")`

---

### Normal vs. Day14 post Folic Acid Treatment

```{r out5, fig.width=11, fig.height=6}
sig.results[[6]]$qqplots
sig.results[[6]]$DEplots
```
**QQ and M-A plots highlighting genes with significant differential expression**  

**Table of transcripts showing significant (FDR<0.2) differential expression** 
```{r tables5, results='asis' }
if (nrow(sig.results[[6]]$results)>(numsig-1)) {
  out <- xtable(sig.results[[6]]$results[1:numsig,])
  } else {
    out  <- xtable(sig.results[[6]]$results)
    }
print(out, type='html',include.rownames=FALSE)
write.table(sig.results[[6]]$results, file=file.path(resultsDir, paste("DE.genes.q", pvalcutoff, setnames[6], "xls", sep=".")), quote=F, sep="\t", row.names=F, col.names=T)
```

[Significant results](editlink
./results/deseq/`r paste("DE.genes.q", pvalcutoff, setnames[6], "xls", sep=".")`


---

### Normal vs. Day28 post Folic Acid Treatment

```{r out6, fig.width=11, fig.height=6}
sig.results[[4]]$qqplots
sig.results[[4]]$DEplots
```
**QQ and M-A plots highlighting genes with significant differential expression**  

**Table of transcripts showing significant (FDR<0.2) differential expression** 
```{r tables6, results='asis' }
if (nrow(sig.results[[4]]$results)>(numsig-1)) {
  out <- xtable(sig.results[[4]]$results[1:numsig,])
  } else {
    out  <- xtable(sig.results[[4]]$results)
    }
print(out, type='html',include.rownames=FALSE)
write.table(sig.results[[4]]$results, file=file.path(resultsDir, paste("DE.genes.q", pvalcutoff, setnames[4], "xls", sep=".")), quote=F, sep="\t", row.names=F, col.names=T)
```


[Significant results](editlink
./results/deseq/`r paste("DE.genes.q", pvalcutoff, setnames[4], "xls", sep=".")`)


---


##Links

[^1]: [Tophat2](http://tophat.cbcb.umd.edu)
[^2]: [iGenomes](http://tophat.cbcb.umd.edu/igenomes.html)
[^3]: [htseq-count](http://www-huber.embl.de/users/anders/HTSeq/doc/count.html)
[^4]: [DESeq](http://bioconductor.org/packages/2.11/bioc/html/DESeq.html)


##Output all expression data and statistics for all 6 comparisons (not just for the significantly DE genes) 
```{r output_all}
counts.output <- as.data.frame(filt.norm.counts)
counts.output$id <- row.names(counts.output)

for (n in 1:6){
  stats.output <- sig.results[[n]]$res.filt
  output <- merge(counts.output,stats.output)
  output <- annotate_df(output)
  output.filename <- file.path(resultsDir, paste("all.exprs.stats", setnames[n], "xls", sep="."))
  write.table(output, file=output.filename, quote=F, sep="\t", row.names=F, col.names=T)
  }

## now forÂ all the genes, not just the filtered genes 
counts.output <- as.data.frame(norm.counts)
counts.output$id <- row.names(counts.output)
for (n in 1:6) {
  stats.output <- sig.results.unfilt[[n]]$res.unfilt
  output <- merge(counts.output,stats.output)
  output <- annotate_df(output)
  output.filename <- file.path(resultsDir, paste("all.exprs.stats.unfiltered.genes", setnames[n], "xls", sep="."))
  write.table(output, file=output.filename, quote=F, sep="\t", row.names=F, col.names=T)  
  }
```
  



```{r save_image}
save.image(file.path(resultsDir, "RDATA" ))
```


```{r aggregate_results}
# pull together the ids of all genes that are differentially expressed relative to the normal samples (at any time point) and get their expression levels over time
# get IDs and counts for  significant DE gen=es
pvalcutoff=0.5
logFCcutoff=1
sig.IDs <- unique(unlist(lapply(sig.results, function(n) n$res.filt$id[which(n$res.filt$padj<pvalcutoff & abs(n$res.filt$log2FoldChange)>logFCcutoff)])))
counts.sig.IDs <- as.data.frame(filt.norm.counts[sig.IDs,])
#reorder columns by time point and replicate
counts.sig.IDs <- counts.sig.IDs[,c(3:1,18:16,12:4,21:19,15:13)]

# make vector of time points to use in median aggregation
time.points <- as.character(c(rep(0,3), rep(1,3), rep(2,3), rep(3,3), rep(7,3), rep(14,3), rep(28,3)))
# transform dataframe to prepare for median aggregation
counts.sig.IDs <- t(counts.sig.IDs)
# median aggregate by time point, label time points as row "time" in new dataframe
counts.sig.IDs.median <- as.data.frame(t(aggregate(counts.sig.IDs, by=list(time=time.points), function(n) median(n))))
# move time values to column names and erase row "with time "time""
names(counts.sig.IDs.median) <- as.character(unlist(counts.sig.IDs.median["time",]))
counts.sig.IDs.median <- counts.sig.IDs.median[-(grep("time", row.names(counts.sig.IDs.median))),]
# reorder columns numerically
counts.sig.IDs.median <- counts.sig.IDs.median[,order(as.numeric(names(counts.sig.IDs.median)))]
counts.sig.IDs.median$id <- row.names(counts.sig.IDs.median) 
counts.sig.IDs.median <- annotate_df(counts.sig.IDs.median)
counts.sig.IDs.median <- counts.sig.IDs.median[,c(9, 2:8)]


write.table(counts.sig.IDs.median, file.path(resultsDir, "normalized.median.counts.for.pairwise.DE.genes.forstem.tab"), quote=F, sep="\t", col.names=T, row.names=F)



```

```{r prepstem }
counts.for.stem <- as.data.frame(filt.norm.counts)
counts.for.stem <- counts.for.stem[,c(3:1,18:16,12:4,21:19,15:13)]
counts.for.stem.t <- t(counts.for.stem)
agg.vec <- sub("rep.", "", row.names(counts.for.stem.t))
med.counts.for.stem.t <- aggregate(filt.norm.counts.t, by=list(agg.vec), function(n) median(n))
row.names(med.counts.for.stem.t) <- med.counts.for.stem.t$Group.1
med.counts.for.stem.t <- med.counts.for.stem.t[,-(grep("Group.1", names(med.counts.for.stem.t)))]
med.counts.for.stem <- as.data.frame(t(med.counts.for.stem.t))
names(med.counts.for.stem) <- sub("Normal", 0, sub("FA", "", sub("day", "", names(med.counts.for.stem))))
med.counts.for.stem <- med.counts.for.stem[,order(as.numeric(names(med.counts.for.stem)))]
med.counts.for.stem$id <- row.names(med.counts.for.stem)
med.counts.for.stem <- annotate_df(med.counts.for.stem)
med.counts.for.stem <- med.counts.for.stem[,c(grep("mgi", names(med.counts.for.stem)), grep("[0-9]", names(med.counts.for.stem)))]
write.table(med.counts.for.stem, file.path(resultsDir, "normalized.median.counts.genes.forstem.tab"), quote=F, sep="\t", col.names=T, row.names=F)
```

```{r messing_around_with_NOISeq}
## need to get annotation files sorted
counts <- read.table(file.path(dataDir, "combined.counts"), header=T, row.names=1)
mydata <- readData(data=counts)
```

## maSigPro Trial
```{r maSigPro}
library(maSigPro)
## setup study design matrix
edesign <- pData(cds.filt)
edesign$Time <-   as.numeric(sub("Normal", "0", sub("day", "", sub("FA", "", sub("rep.", "", row.names(edesign))))))
edesign$Replicate <- sub("rep", "", sub("FA.+day", "", sub("Normal", "", row.names(pData(cds.filt)))))
edesign$sizeFactor <- NULL
edesign$condition <- ifelse(grepl("FA", edesign$condition), "Treated", "Control")
edesign$Control <- ifelse(edesign$condition=="Control", 1, 0)
edesign$Treated <- ifelse(edesign$condition=="Treated", 1, 0)
edesign$condition <- NULL
edesign <- as.matrix(edesign)
class(edesign) <- "numeric"
identical(dimnames(filt.norm.counts)[[2]], dimnames(edesign)[[1]])
## define the regression model
## make degrees of freedom to by one less than the number of time points
design <- make.design.matrix(edesign, degree=6)
library(MASS)
# will get step sizwise errors  with larger degrees of freedom unless you remove all zero-inflated (any row with any zeroes in the Treated datapoints) data
temp <- filt.norm.counts[!(apply(filt.norm.counts, 1,function(n) any(n[4:21]==0))),]
## use negative binomial distribution for model, but hard to know what to set theta as...
NGS.p <- p.vector(temp, design, family=negative.binomial(1))
NGS.t <- T.fit(NGS.p, step.method="two.ways.backward")


