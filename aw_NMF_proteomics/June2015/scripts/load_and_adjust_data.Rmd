---
output:
  html_document:
    toc: true
    toc_depth: 3
    theme: journal
css: custom.css
title: "NMF analysis of proteomic data - Set1"
theme: united
---

last update `r date()`

```{r setupstate, echo=FALSE}
library(knitr)
opts_chunk$set(tidy=TRUE, cache=TRUE,  highlight=TRUE, figalign="center", echo=TRUE, warning=FALSE, error=FALSE, message=FALSE, tidy.opts=list(keep.blank.line=FALSE, width.cutoff=120))
options(width=200,bitmapType = 'cairo')
# render_2_drop("from_john.Rmd","proteomics")
```

# SETUP

## Directories and Variables
- either local or using Odyssey

```{r setup}
if(file.exists("/Users/johnhutchinson/Work/projects")){
  baseDir  <- "/Users/johnhutchinson/Work/projects/aw_NMF_proteomics/June2015/"
  } else if (file.exists("/n/home08/")){
    baseDir <- "/n/hsphS10/hsphfs1/chb/projects/aw_NMF_proteomics/June2015"
    }
dataDir <- file.path(baseDir, "data")
resultsDir <- file.path(baseDir, "results")
metaDir <- file.path(baseDir, "meta")
```

## Libraries

```{r libraries}
library(reshape2)
library(ggplot2)
library(readr)
library(dplyr)
library(pheatmap)
library(ggdendro)
library(CHBUtils)
library(edgeR)
library(plyr)
library(RUVSeq)
library(limma)
library(knitr)
library(NMF)
```

## Load Data

There are three separate datasets, each with the same data classes in each.

```{r loaddata}
rawdata.set0 <- read.csv(file.path(dataDir, "Set0", "Set0-Summary-2.csv"))
rawdata.set1 <- read.csv(file.path(dataDir, "Set1", "Set1-Summary-2.csv"))
rawdata.set2 <- read.csv(file.path(dataDir, "Set2", "Set2-Summary-2.csv"))
```

# Data Summarize and Explore

## Summarize/Collapse all identical phospho-residues
- same phospho-proteomic site can be represented in the data by a different peptide
- assuming they are all coming from the same origin, makes sense to just sum the values for identical phosphoresidues found on same proteins

```{r summarizephospresidues}
sumdata.set0 <- aggregate(cbind(Summed.126, Summed.127, Summed.128, Summed.129, Summed.130, Summed.131) ~ GeneName+Protein.Relative.Modifications.1, data=rawdata.set0, sum)
sumdata.set1 <- aggregate(cbind(Summed.126, Summed.127, Summed.128, Summed.129, Summed.130, Summed.131) ~ GeneName+Protein.Relative.Modifications.1, data=rawdata.set1, sum)
sumdata.set2 <- aggregate(cbind(Summed.126, Summed.127, Summed.128, Summed.129, Summed.130, Summed.131) ~ GeneName+Protein.Relative.Modifications.1, data=rawdata.set2, sum)
```

## Data Clean
- some lines have no signal, dropped them

```{r dataclean}
# remove any rows with zero counts for all samples
sumdata.set0 <- sumdata.set0[!(apply(sumdata.set0[, grep("Summed", names(sumdata.set0))], 1, function(x) all(x==0))),]
sumdata.set1 <- sumdata.set1[!(apply(sumdata.set0[, grep("Summed", names(sumdata.set1))], 1, function(x) all(x==0))),]
sumdata.set2 <- sumdata.set2[!(apply(sumdata.set0[, grep("Summed", names(sumdata.set2))], 1, function(x) all(x==0))),]
```

## Data exploration
- exploring best way to combine the different sets of data
- not all sets detected the same phosphoproteomic sites
- does it make a difference is we merge the datasets using only common phosphoproteomic sites or should we use all of them?

```{r dataexplore}
# merge datasets together - all phospho sites
sumdata.set.0.1. <- merge(sumdata.set0, sumdata.set1, by=c("GeneName", "Protein.Relative.Modifications.1"), suffixes=c(".0", ".1"), all=TRUE)
sumdata.set.0.1.2 <- merge(sumdata.set.0.1., sumdata.set2, by=c("GeneName", "Protein.Relative.Modifications.1"), all=TRUE)
sumdata <- sumdata.set.0.1.2
names(sumdata)[(ncol(sumdata)-5):ncol(sumdata)] <- paste(names(sumdata)[(ncol(sumdata)-5):ncol(sumdata)], ".2", sep="")
sumdata.m <- melt(sumdata, id.vars=c("GeneName", "Protein.Relative.Modifications.1"))
ggplot(sumdata.m, aes(x=value, col=variable) )+geom_density()+scale_x_log10()+ggtitle("Summed counts, all phospho sites")


# merge datasets together - only common phospho sites
sumdata.set.0.1. <- merge(sumdata.set0, sumdata.set1, by=c("GeneName", "Protein.Relative.Modifications.1"), suffixes=c(".0", ".1"))
sumdata.set.0.1.2 <- merge(sumdata.set.0.1., sumdata.set2, by=c("GeneName", "Protein.Relative.Modifications.1"))
sumdata <- sumdata.set.0.1.2
names(sumdata)[(ncol(sumdata)-5):ncol(sumdata)] <- paste(names(sumdata)[(ncol(sumdata)-5):ncol(sumdata)], ".2", sep="")
sumdata.m <- melt(sumdata, id.vars=c("GeneName", "Protein.Relative.Modifications.1"))
ggplot(sumdata.m, aes(x=value, col=variable) )+geom_density()+scale_x_log10()+ggtitle("Summed counts, common phospho sites")
```

- really can't see any reason to keep all the sites for this, might be important if we decide to use the edgeR method of normalization?
- going forward with just the common sites before normalization

# Normalize Data

## Normalize total counts for each sample
- first try simple method based on adjusting for total output for each sample
- scale values of samples so all samples have same total intensity scores for common phospho sites

```{r normvalues}
# using just total count norm
## merge datasets together - only common phospho sites
sumdata.set.0.1. <- merge(sumdata.set0, sumdata.set1, by=c("GeneName", "Protein.Relative.Modifications.1"), suffixes=c(".0", ".1"))
sumdata.set.0.1.2 <- merge(sumdata.set.0.1., sumdata.set2, by=c("GeneName", "Protein.Relative.Modifications.1"))
sumdata <- sumdata.set.0.1.2
names(sumdata)[(ncol(sumdata)-5):ncol(sumdata)] <- paste(names(sumdata)[(ncol(sumdata)-5):ncol(sumdata)], ".2", sep="")
## calc total intensities for each sample
colsums <- colSums(sumdata[, grep("Summed", names(sumdata))])
## calc multiplier modifier for each samples, based on sample with total lowest intensity
mods<- 1/(colsums/min(colsums))
## normalize summed data with multiplier modifier
normed.sums <- as.data.frame(t(t(sumdata[,grep("Summed", names(sumdata))])*mods))
## add normalized data to non-normed data
names(normed.sums) <- sub("Summed", "Normed", names(normed.sums))
data <- cbind(sumdata, normed.sums)
normed.data <- cbind(data[,1:2], data[,grep("Normed", names(data))])
normed.data$gene_phosphosite <- paste(normed.data$GeneName, normed.data$Protein.Relative.Modifications.1, sep="_")
row.names(normed.data) <- normed.data$gene_phosphosite
normed.data <- log2(normed.data[,grep("Normed", names(normed.data))] + 0.5)

# plot normed results
normed.data.m <- melt(normed.data)

ggplot(normed.data.m, aes(x=value, col=variable) )+geom_density()
heatmap(as.matrix(normed.data), labRow = NA)
myDist <- dist(t(1-cor(normed.data)))
myTree <- hclust(myDist, method = "ward.D2")
dhc <- as.dendrogram(myTree)
ggdendrogram(dhc)
```

The data is still clustering more by batch than by sample class. 

## Using edgeR
- using  a library deigned for count data from RNA-seq, trying to see if it is effective for phosph-proteomic results
- adjusts for both total "library" size and RNA composition effects i.e. will compensate for outlier RNAs that might use up a significant proportion of the results, to avoid undersampling of of other genes in the sample
- here I used all the phosphoproteomic sites, as edgeR adjusts for "RNA" composition, and excluding "RNAs" might skew that in unexpected ways

>The calcNormFactors function normalizes for RNA composition by finding a set of scaling factors for the library sizes that minimize the log-fold changes between the samples for most genes.

```{r edgeRnorm}
# merge datasets together - all phospho sites
sumdata.set.0.1. <- merge(sumdata.set0, sumdata.set1, by=c("GeneName", "Protein.Relative.Modifications.1"), suffixes=c(".0", ".1"), all=TRUE)
sumdata.set.0.1.2 <- merge(sumdata.set.0.1., sumdata.set2, by=c("GeneName", "Protein.Relative.Modifications.1"), all=TRUE)
sumdata <- sumdata.set.0.1.2
names(sumdata)[(ncol(sumdata)-5):ncol(sumdata)] <- paste(names(sumdata)[(ncol(sumdata)-5):ncol(sumdata)], ".2", sep="")

# prep for edgeR import
phosphosites <- paste(sumdata[,1], sumdata[,2], sep="_")
sumdata <- cbind(phosphosites, sumdata[, grep("umm", names(sumdata))])
row.names(sumdata) <- sumdata$phosphosites
sumdata$phosphosites <- NULL
sumdata[is.na(sumdata)] <- 0

# import into edgeR object and calc norm factors
sumdata.dge <- DGEList(counts=sumdata)
sumdata.dge <- calcNormFactors(sumdata.dge)

# output normed data adjusted for library size and
normed.data.edger <- cpm(sumdata.dge, normalized.lib.sizes = TRUE, log = TRUE)

# subset to commmon phospho sites
normed.data.edger <- normed.data.edger[row.names(normed.data.edger) %in% row.names(normed.data),]
normed.data.edger <- as.data.frame(normed.data.edger)

# plot normed results
normed.data.edger.m <- melt(normed.data.edger)

ggplot(normed.data.edger.m, aes(x=value, col=variable) )+geom_density()
heatmap(as.matrix(normed.data.edger), labRow = NA)
myDist <- dist(t(1-cor(normed.data.edger)))
myTree <- hclust(myDist, method = "ward.D2")
dhc <- as.dendrogram(myTree)
ggdendrogram(dhc)
```

Both normlaization methods are insufficient to overcome the batch run effect ie. they are still clustering by batch. While we could account for batch in the differential "expression" equation we can't do this for NMF, we instead require a pre-adjusted matrix.

# Batch correction

Using  [RUVseq](http://www.nature.com/nbt/journal/v32/n9/full/nbt.2931.html)

Batch here corresponds to "Set".
For sample classes, you can either load in the metadata, or just work off the sample numbers.

resting bone marrow == RBM == unmobilized ==  D1
peripheral mobilized == PM == mobilized_spleen ==  D4


```{r setupmetadata}
metadata <- laply(strsplit(names(normed.data), "\\."), function(x) {
  sample <- x[2]
  batch <- x[3]
  return(list(sample=sample, batch=batch))
})
metadata <- as.data.frame(metadata)
metadata$sampleclass <- ifelse(metadata$sample==126| metadata$sample==127 | metadata$sample==128, "D1", "D4")
```


```{r RUVseq, eval=FALSE}
set = newSeqExpressionSet(as.matrix(normed.data.edger))
difference <- matrix(data=c(c(1:3,7:9,13:15), c(4:6,10:12,16:18)), byrow=TRUE, nrow=2)
batch_ruv_emp <- RUVs(as.matrix(normed.data.edger), rownames(normed.data.edger), k=2, difference)

normed.suv <- as.data.frame(log2(batch_ruv_emp$normalizedCounts + 0.5))

normed.suv.m <- melt(normed.suv)

ggplot(normed.suv.m, aes(x=value, col=variable) )+geom_density()
heatmap(as.matrix(normed.suv), labRow = NA)
myDist <- dist(t(1-cor(normed.suv)))
myTree <- hclust(myDist, method = "ward.D2")
dhc <- as.dendrogram(myTree)
ggdendrogram(dhc)
```

```{r loadbatchcorrected, echo=FALSE}
load(file.path(resultsDir, "batch.corrected.counts"))
```


# Differential "Expression"
 - using [limma voom](http://www.genomebiology.com/2014/15/2/R29)

```{r limma, results='asis'}
row.names(metadata) = names(sumdata.dge$counts)
dd = cbind(metadata, batch_ruv_emp$W)[,1:5]
dd$batch = as.factor(unlist(dd$batch))  

ma = model.matrix(~ 0 + sampleclass + batch, data=dd)
  
dat.voom = voom(batch_ruv_emp$normalizedCounts,design = ma, plot = TRUE)
dat.fit <- lmFit(dat.voom, ma)

cont.ma = makeContrasts(condition=sampleclassD4-sampleclassD1, levels=ma)
dat.fit.cont <- contrasts.fit(dat.fit, cont.ma)
dat.bayes <- eBayes(dat.fit.cont)

kable(summary(decideTests(dat.bayes)))

all_de = topTable(dat.bayes, coef="condition", number = Inf)
```

Table above shows how many phospho-proteomics sites are differentially "expresssed" between the cell types. 

## Output to files

```{r output}
write.csv(normed.suv, file=file.path(resultsDir, "normalized.data.suvseq.csv"))
write.csv(all_de, file=file.path(resultsDir, "limma.voom.csv"))
# write.csv(normed.data.edger, file=file.path(dataDir, "Set2", "normalized.data.edgeR.csv"))
```

[normalized matrix](../results/normalized.data.suvseq.csv)

[differential expression](../results/limma.voom.csv)


# NMF Analysis
- finding "metagenes"
- using normalized and batch corrected data

## NMF Variables

```{r nmfvars}
TRAIN=50
RUN=1250
mad.cutoff=0.5

minnumfeatures=25
```


The first four components explain ~87.5% of the sample differences for both approaches to the data, and when compared pairwise,  the first two components easily separate out the cell types.

## NMF Preparation

### Estimating the factorization rank
From ["An introduction to NMF package"](http://nmf.r-forge.r-project.org/vignettes/NMF-vignette.pdf)

>A critical parameter in NMF is the factorization rank r. It determines the number of metagenes used
to approximate the target matrix. Given a NMF method and the target matrix, a common way of
deciding on r is to try different values, compute some quality measure of the results, and choose
the best value according to this quality criteria.

#### Using metrics:

From ["An introduction to NMF package"](http://nmf.r-forge.r-project.org/vignettes/NMF-vignette.pdf (NMF vignette))

>(Brunet et al. 2004) proposed to take the first value of r for which the cophenetic coefficient starts
decreasing, (Hutchins et al. 2008) suggested to choose the first value where the RSS curve presents
an inflection point, and (Frigyesi et al. 2008) considered the smallest value at which the decrease
in the RSS is lower than the decrease of the RSS obtained from random data.

```{r estimrank}
# drop weird rows with all negative numbers or only zeroes
eset.corr <- normed.suv
eset.corr <- eset.corr[apply(eset.corr, 1, function(x) all(x>0)),]

groups.corr <-  as.factor(metadata$sampleclass)

estim.corr <- nmf(eset.corr, 2:5, nrun = TRAIN, seed = 123456, .options='v') #allow parallel compute
plot(estim.corr)
```

These results suggest that 2 metagenes might separate all the classes as there isn't a clear  inflection point for the rss or cophenetic curves.

Even for random data, increasing factorization ranks lead to more variables to fit the data, possibly leading to overfitting the data. Too assess this, we can run the same metrics after randomly permuting the data, and comparing the performance of the original and randomized datasets. 

```{r overfitcheck, results='hide',warning=FALSE, message=FALSE, error=FALSE,cache=TRUE}
# shuffle original data to look for overfitting
eset.corr.rand <- randomize(eset.corr)
# estimate quality measures from the shuffled data (use default NMF algorithm)
estim.corr.rand <- nmf(eset.corr.rand, 2:5, nrun = TRAIN, seed = 12345, .options="v")
# plot measures on same graph
plot(estim.corr, estim.corr.rand)
```

In the plots above, the solid lines represent the actual data while the dotted lines represent the same data after random shuffling by permuting the rows of each column, using a different permutation each time. These results show that overfitting is not an issue.

#### Qualitative assessment

We can also examine heatmaps of the consensus matrix for each value of the factorization rank to see if the clusters (or consensus blocks) obtained correspond to the known cell types.

```{r estimatefactoriziationrank.qualitative, results='hide'}
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

consensusmap(estim.corr, annCol=list(samplegroups=as.character(unlist(groups.corr))),annColors=list(samplegroups=cbPalette[1:3]), labCol=groups.corr, labRow=groups.corr, scale="row", color='-RdYlBu2:200')
```

You can clearly see that 2 metagenes separate the groups well.

### Choosing an algorithm

To this point, I have been using the default (*brunet*) algorithm for the gross tuning of NMF; we can further fine tune NMF by choosing an alternate algorithm.
 
To do so, I ran NMF on the data using four of the most popular algorithms (with error tracking enabled), and compared the error plots. In these error plots, each track is normalized so that its first value equals one, and stops at the iteration where the method's convergence criterion was fulfilled; algorithms that converge at lower objective values are optimal.

```{r comparealgs, cache=TRUE, results="hide", message=FALSE, error=FALSE,warning=FALSE}
res.multi.method.2 <- nmf(eset.corr, 2, list("brunet", "KL", "lee","nsNMF"), nrun=TRAIN, seed = 123456, .options = "tv")
plot(res.multi.method.2, main="NMF residuals - 2 metagenes")
```

Here, the *nsMMF* algorirthm works best. The nsNMF approach has the slight benefit of creating 'sparse' metagenes which are easier to interpret functionally, so I proceeded with this algorithm.

## Complete NMF Algorithm

The NMF analyses so far have used a low number of iterations (<50) so may not be completely accurate,. Here I ran NMF with `r RUN` iterations to allow the algorithm to fully converge and yield as accurate a result as possible.

```{r fullNMF, eval=TRUE}
res.final.2 <- nmf(eset.corr, 2, "nsNMF", nrun=RUN, .options = "tv")

# save precomputed NMF values, hack to avoid using caching
save(list="res.final.2", file=file.path(resultsDir,  "RDATA.res.final.2"))
```

```{r fullNMFload, echo=FALSE}
# load precomputed NMF values, hack to avoid using caching
load(file.path(resultsDir,  "RDATA.res.final.2"))
```

We can look at the final results in various ways:

The consensus matrix plot is similar to the previous plot, just with more iterations. It shows how well the calculated metagenes cluster the samples.

```{r rank2consensumap}
consensusmap(res.final.2,  annCol=list(samplegroups=as.character(unlist(groups.corr))),  labCol=groups.corr, labRow=groups.corr, annColors=list(samplegroups=cbPalette[1:2], basis=cbPalette[5:6], consensus=brewer.pal(3, "Spectral")))
```

### NMF Metagene Feature Extraction

The "Metagene Components"" plots indicate the contributions of a gene to the individual metagenes. 

Here I first plotted all genes that contribute to any metagene.

```{r rank2basismap, cache=TRUE}
basismap(res.final.2, scale="r1",annColors=list(basis=cbPalette[5:6], consensus=brewer.pal(2, "Spectral")), main="Metagene Components - All Contributing Genes") # shows metagenes in relation to samples
```

#### All NMF Metagene Features

We can extract the genes that make a contribution to the different metagenes. In the Excel files below, I included all genes that contribute to a metagene.

```{r rank2.metagenes}
fs2 <- featureScore(res.final.2) # get all the feature scores which measure specificity to which a gene contributes to a metagene
f2 <- extractFeatures(res.final.2) # extract the the 50 most features with the highest specifciity for each of the metagenes

# dataframe to present the metagene features that includes both the metagene annotations and the featurescores
metagene2.1 <- cbind(fData(eset.corr)[f2[[1]],], fs2[f2[[1]]]) 
names(metagene2.1)[ncol(metagene2.1)] <- "featurescore"
metagene2.2 <- cbind(fData(eset.corr)[f2[[2]],], fs2[f2[[2]]]) 
names(metagene2.2)[ncol(metagene2.2)] <- "featurescore"

write.table(as.matrix(metagene2.1), file=file.path(resultsDir, paste("metagene", 1, "xls", sep=".")), sep="\t", quote=F)
write.table(as.matrix(metagene2.2), file=file.path(resultsDir, paste("metagene", 2, "xls", sep=".")), sep="\t", quote=F)

## get unique annotations for genes (NMF returns number referenced IDs) in metagenes
unique.metagenesymbols <- lapply(f2, function(x) {
					   genenames <- unique(unlist(fData(eset.corr)[x,"GeneName"]))
					     return(genenames)
					     })
## get number of unique genes in each metagene
numgenes <- unlist(lapply(unique.metagenesymbols, length))
```

[Excel file of metagene number 1 features](../results/6plex_1/metagene.1.xls)  
[Excel file of metagene number 2 features](../results/6plex_1/metagene.2.xls)  

